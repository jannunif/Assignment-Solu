{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e5fd1d-1270-4c33-9826-dc66ade73ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans01 : Simple linear regression is a statistical method used to study the relationship between two variables where one variable is dependent on the other. It assumes a linear relationship between the two variables, meaning that a change in one variable is directly proportional to a change in the other. In simple linear regression, only one independent variable is used to predict the dependent variable.\n",
    "#        For example, let's say we want to study the relationship between the number of hours a student studies and their exam score. Here, the number of hours studied is the independent variable, and the exam score is the dependent variable. By analyzing the data, we can determine the linear relationship between the two variables and use this information to predict the exam score of a student based on the number of hours they studied.\n",
    "#        Multiple linear regression, on the other hand, is used when there are more than one independent variable that influences the dependent variable. It examines the relationship between the dependent variable and multiple independent variables simultaneously, and assumes that the relationship between the dependent variable and each independent variable is linear.\n",
    "#        For example, let's say we want to study the factors that affect the price of a house. Here, there can be multiple independent variables that affect the house price, such as the size of the house, number of bedrooms, location, and so on. By using multiple linear regression, we can determine the combined effect of all these factors on the price of the house and use this information to predict the price of a house given its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593e4b54-1f84-45a7-8a27-fe9f2baadab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear regression is a statistical method that makes several assumptions about the data in order to produce reliable estimates of the model parameters and predictions. Here are the main assumptions of linear regression:\\n\\n1)Linearity           : The relationship between the dependent variable and each independent variable is linear.\\n2)Independence        : The observations are independent of each other.\\n3)Homoscedasticity    : The variance of the errors is constant across all levels of the independent variables.\\n4)Normality           : The errors are normally distributed.\\n5)No multicollinearity: The independent variables are not highly correlated with each other.\\n\\nThere are several methods to check whether these assumptions hold in a given dataset. Here are a few of them:\\n\\n1)Scatter plots                  : A scatter plot can be used to visually check the linearity assumption by plotting the dependent variable against each independent variable.\\n2)Residual plots                 : Residual plots can be used to check the assumptions of homoscedasticity and normality. A plot of the residuals against the fitted values can show whether the variance of the errors is constant across all levels of the independent variables, and a histogram or a normal probability plot of the residuals can show whether they are normally distributed.\\n3)Durbin-Watson test             : The Durbin-Watson test can be used to check the independence assumption by testing for autocorrelation in the residuals.\\n4)Variance Inflation Factor (VIF): The VIF can be used to check for multicollinearity by examining the correlation between the independent variables. A VIF greater than 5 or 10 indicates a high degree of multicollinearity.\\n5)It is important to check the assumptions of linear regression before interpreting the results or making predictions based on the model. If the assumptions do not hold, the model may produce biased or unreliable estimates of the parameters and predictions. If any of the assumptions are violated, there are several techniques to address the issue, such as transforming the variables, using a different model, or excluding outliers or influential observations.\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans02 : \n",
    "\"\"\"\n",
    "Linear regression is a statistical method that makes several assumptions about the data in order to produce reliable estimates of the model parameters and predictions. Here are the main assumptions of linear regression:\n",
    "\n",
    "1)Linearity           : The relationship between the dependent variable and each independent variable is linear.\n",
    "2)Independence        : The observations are independent of each other.\n",
    "3)Homoscedasticity    : The variance of the errors is constant across all levels of the independent variables.\n",
    "4)Normality           : The errors are normally distributed.\n",
    "5)No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "\n",
    "There are several methods to check whether these assumptions hold in a given dataset. Here are a few of them:\n",
    "\n",
    "1)Scatter plots                  : A scatter plot can be used to visually check the linearity assumption by plotting the dependent variable against each independent variable.\n",
    "2)Residual plots                 : Residual plots can be used to check the assumptions of homoscedasticity and normality. A plot of the residuals against the fitted values can show whether the variance of the errors is constant across all levels of the independent variables, and a histogram or a normal probability plot of the residuals can show whether they are normally distributed.\n",
    "3)Durbin-Watson test             : The Durbin-Watson test can be used to check the independence assumption by testing for autocorrelation in the residuals.\n",
    "4)Variance Inflation Factor (VIF): The VIF can be used to check for multicollinearity by examining the correlation between the independent variables. A VIF greater than 5 or 10 indicates a high degree of multicollinearity.\n",
    "5)It is important to check the assumptions of linear regression before interpreting the results or making predictions based on the model. If the assumptions do not hold, the model may produce biased or unreliable estimates of the parameters and predictions. If any of the assumptions are violated, there are several techniques to address the issue, such as transforming the variables, using a different model, or excluding outliers or influential observations.\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfb428e-ffe7-4972-860f-395026ccdbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn a linear regression model, the slope and intercept are the estimated parameters of the line of best fit. The slope represents the change in the dependent variable for every unit increase in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is equal to zero.\\nFor example, let's say we have a linear regression model that predicts the sales of a product based on the amount spent on advertising. The model is given by:\\nSales = 100 + 2 * Advertising\\nHere, the intercept is 100, which means that if we spend zero dollars on advertising, we can still expect to sell 100 units of the product. The slope is 2, which means that for every dollar spent on advertising, we can expect to sell an additional two units of the product.\\nTo interpret these parameters, we can say that the intercept represents the baseline sales that we can expect without any advertising, while the slope represents the additional sales we can expect for every dollar spent on advertising. Therefore, we can use this model to predict the sales of the product given a certain advertising budget.\\nFor example, if we spend $10,000 on advertising, we can predict the sales as follows:\\nSales = 100 + 2 * 10,000 = 20,100 units\\nThis means that we can expect to sell 20,100 units of the product if we spend $10,000 on advertising, assuming that the model is accurate and the assumptions are met.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans03 : \n",
    "\"\"\"\n",
    "In a linear regression model, the slope and intercept are the estimated parameters of the line of best fit. The slope represents the change in the dependent variable for every unit increase in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is equal to zero.\n",
    "For example, let's say we have a linear regression model that predicts the sales of a product based on the amount spent on advertising. The model is given by:\n",
    "Sales = 100 + 2 * Advertising\n",
    "Here, the intercept is 100, which means that if we spend zero dollars on advertising, we can still expect to sell 100 units of the product. The slope is 2, which means that for every dollar spent on advertising, we can expect to sell an additional two units of the product.\n",
    "To interpret these parameters, we can say that the intercept represents the baseline sales that we can expect without any advertising, while the slope represents the additional sales we can expect for every dollar spent on advertising. Therefore, we can use this model to predict the sales of the product given a certain advertising budget.\n",
    "For example, if we spend $10,000 on advertising, we can predict the sales as follows:\n",
    "Sales = 100 + 2 * 10,000 = 20,100 units\n",
    "This means that we can expect to sell 20,100 units of the product if we spend $10,000 on advertising, assuming that the model is accurate and the assumptions are met.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca253c2-08dd-48b1-ba16-9bd7296eb5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient descent is an optimization algorithm used to minimize the cost function in a machine learning model. It works by iteratively adjusting the model parameters in the direction of the steepest descent of the cost function. The idea is to start with an initial set of parameters and update them in small steps until the cost function reaches its minimum value, which represents the best possible fit of the model to the data.\\nThe steps involved in the gradient descent algorithm are as follows:\\n\\nInitialize the parameters  : Choose an initial set of parameters, typically at random.\\n1)Calculate the cost function: Evaluate the cost function using the current set of parameters.\\n2)Calculate the gradients    : Calculate the partial derivatives of the cost function with respect to each parameter, which represent the direction and magnitude of the steepest descent.\\n3)Update the parameters      : Adjust the parameters by subtracting a fraction of the gradients from their current values. The fraction is known as the learning rate and determines how large or small the steps are.\\n4)Repeat steps 2 to 4        : Continue iterating the process until the cost function reaches its minimum value or a specified number of iterations is reached.\\nGradient descent is widely used in machine learning to train models, such as linear regression, logistic regression, neural networks, and others. In these models, the cost function is usually the sum of the squared errors between the predicted and actual values of the dependent variable. By using gradient descent to minimize the cost function, the model can be optimized to make more accurate predictions on new data.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans04 : \n",
    "\"\"\"\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in a machine learning model. It works by iteratively adjusting the model parameters in the direction of the steepest descent of the cost function. The idea is to start with an initial set of parameters and update them in small steps until the cost function reaches its minimum value, which represents the best possible fit of the model to the data.\n",
    "The steps involved in the gradient descent algorithm are as follows:\n",
    "\n",
    "Initialize the parameters  : Choose an initial set of parameters, typically at random.\n",
    "1)Calculate the cost function: Evaluate the cost function using the current set of parameters.\n",
    "2)Calculate the gradients    : Calculate the partial derivatives of the cost function with respect to each parameter, which represent the direction and magnitude of the steepest descent.\n",
    "3)Update the parameters      : Adjust the parameters by subtracting a fraction of the gradients from their current values. The fraction is known as the learning rate and determines how large or small the steps are.\n",
    "4)Repeat steps 2 to 4        : Continue iterating the process until the cost function reaches its minimum value or a specified number of iterations is reached.\n",
    "Gradient descent is widely used in machine learning to train models, such as linear regression, logistic regression, neural networks, and others. In these models, the cost function is usually the sum of the squared errors between the predicted and actual values of the dependent variable. By using gradient descent to minimize the cost function, the model can be optimized to make more accurate predictions on new data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cc75bc-9f9c-406f-baf1-884cff5f487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMultiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and two or more independent variables. In multiple linear regression, we assume that the dependent variable is a linear function of the independent variables, each of which has its own slope coefficient, along with a constant intercept term.\\nThe multiple linear regression model can be expressed mathematically as:\\n\\ny = β0 + β1x1 + β2x2 + ... + βpxp + ε\\n\\nwhere:\\n\\ny is the dependent variable\\nβ0 is the intercept term\\nβ1, β2, ..., βp are the coefficients of the independent variables x1, x2, ..., xp, respectively\\nε is the error term\\nThe difference between multiple linear regression and simple linear regression is that simple linear regression models the relationship between one independent variable and one dependent variable, while multiple linear regression models the relationship between multiple independent variables and one dependent variable.\\n\\nIn other words, while simple linear regression produces a straight line that represents the relationship between the two variables, multiple linear regression produces a hyperplane that represents the relationship between the multiple independent variables and the dependent variable. This allows us to capture more complex relationships between the variables and account for the influence of multiple factors on the dependent variable.\\nFor example, in a simple linear regression model, we might model the relationship between a person's weight and their height. In contrast, in a multiple linear regression model, we might model the relationship between a person's weight and their height, age, and gender, to account for the fact that these variables may also influence a person's weight.\\nIn summary, multiple linear regression is a statistical model that allows us to model the relationship between a dependent variable and two or more independent variables. It differs from simple linear regression in that it can capture more complex relationships between variables and account for the influence of multiple factors on the dependent variable.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans05 : \n",
    "\"\"\"\n",
    "Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and two or more independent variables. In multiple linear regression, we assume that the dependent variable is a linear function of the independent variables, each of which has its own slope coefficient, along with a constant intercept term.\n",
    "The multiple linear regression model can be expressed mathematically as:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + ... + βpxp + ε\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable\n",
    "β0 is the intercept term\n",
    "β1, β2, ..., βp are the coefficients of the independent variables x1, x2, ..., xp, respectively\n",
    "ε is the error term\n",
    "The difference between multiple linear regression and simple linear regression is that simple linear regression models the relationship between one independent variable and one dependent variable, while multiple linear regression models the relationship between multiple independent variables and one dependent variable.\n",
    "\n",
    "In other words, while simple linear regression produces a straight line that represents the relationship between the two variables, multiple linear regression produces a hyperplane that represents the relationship between the multiple independent variables and the dependent variable. This allows us to capture more complex relationships between the variables and account for the influence of multiple factors on the dependent variable.\n",
    "For example, in a simple linear regression model, we might model the relationship between a person's weight and their height. In contrast, in a multiple linear regression model, we might model the relationship between a person's weight and their height, age, and gender, to account for the fact that these variables may also influence a person's weight.\n",
    "In summary, multiple linear regression is a statistical model that allows us to model the relationship between a dependent variable and two or more independent variables. It differs from simple linear regression in that it can capture more complex relationships between variables and account for the influence of multiple factors on the dependent variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d87a5d-885a-4159-9536-99cbb6a86eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulticollinearity is a common issue that can arise in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause problems in the model, as it makes it difficult to distinguish the separate effects of each variable on the dependent variable. In other words, it becomes hard to determine which independent variable is driving the changes in the dependent variable, and this can lead to inaccurate or unstable coefficient estimates.\\n\\nTo detect multicollinearity, there are several methods that can be used:\\n\\nCorrelation matrix             : Compute a correlation matrix between all pairs of independent variables. If two or more variables have a correlation coefficient close to 1 or -1, this indicates a high degree of collinearity.\\nVariance Inflation Factor (VIF): Compute the VIF for each independent variable, which measures how much the variance of the coefficient estimates is increased due to collinearity. A VIF value greater than 5 or 10 is usually considered high and indicates collinearity.\\nEigenvalues                    : Compute the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero, this indicates that the variables are highly collinear.\\n\\nOnce multicollinearity is detected, there are several ways to address this issue:\\n\\nDrop one of the correlated variables: If two or more variables are highly correlated, one of them can be dropped from the model. The variable that is less important or less relevant can be dropped.\\nCombine the variables               : Instead of using two or more correlated variables separately in the model, they can be combined into a single variable that captures their joint effect. This can be done by creating a new variable through a factor analysis, principal component analysis, or other methods.\\nRidge regression                    : Ridge regression is a technique that adds a penalty term to the regression model to reduce the magnitude of the coefficient estimates. This can help to reduce the impact of multicollinearity on the model.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans06 : \n",
    "\"\"\"\n",
    "Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause problems in the model, as it makes it difficult to distinguish the separate effects of each variable on the dependent variable. In other words, it becomes hard to determine which independent variable is driving the changes in the dependent variable, and this can lead to inaccurate or unstable coefficient estimates.\n",
    "\n",
    "To detect multicollinearity, there are several methods that can be used:\n",
    "\n",
    "Correlation matrix             : Compute a correlation matrix between all pairs of independent variables. If two or more variables have a correlation coefficient close to 1 or -1, this indicates a high degree of collinearity.\n",
    "Variance Inflation Factor (VIF): Compute the VIF for each independent variable, which measures how much the variance of the coefficient estimates is increased due to collinearity. A VIF value greater than 5 or 10 is usually considered high and indicates collinearity.\n",
    "Eigenvalues                    : Compute the eigenvalues of the correlation matrix. If one or more eigenvalues are close to zero, this indicates that the variables are highly collinear.\n",
    "\n",
    "Once multicollinearity is detected, there are several ways to address this issue:\n",
    "\n",
    "Drop one of the correlated variables: If two or more variables are highly correlated, one of them can be dropped from the model. The variable that is less important or less relevant can be dropped.\n",
    "Combine the variables               : Instead of using two or more correlated variables separately in the model, they can be combined into a single variable that captures their joint effect. This can be done by creating a new variable through a factor analysis, principal component analysis, or other methods.\n",
    "Ridge regression                    : Ridge regression is a technique that adds a penalty term to the regression model to reduce the magnitude of the coefficient estimates. This can help to reduce the impact of multicollinearity on the model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1acbb22-1e78-4680-abea-62ef0c66dfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPolynomial regression is a type of regression analysis in which the relationship between the dependent variable and one or more independent variables is modeled as an nth degree polynomial. This means that instead of modeling the relationship with a straight line (as in linear regression), we use a curve or a polynomial function to fit the data.\\nThe polynomial regression model can be expressed mathematically as:\\n\\ny = β0 + β1x + β2x^2 + β3x^3 + ... + βnx^n + ε\\n\\nwhere:\\n\\ny is the dependent variable\\nx is the independent variable\\nβ0, β1, β2, ..., βn are the coefficients of the polynomial terms, with β0 being the intercept term\\nε is the error term\\n\\nThe difference between polynomial regression and linear regression is that linear regression models the relationship between the dependent variable and the independent variable as a straight line, while polynomial regression models the relationship as a curved line or a polynomial function.\\n\\nPolynomial regression can capture more complex relationships between the independent and dependent variables compared to linear regression. For example, if the data points are best fit with a curved line, polynomial regression can capture this relationship more accurately than a straight line in linear regression. This makes it a useful tool for modeling nonlinear relationships.\\nOne potential downside of polynomial regression is that it can be prone to overfitting. This means that the model may fit the training data too closely, which can lead to poor performance on new, unseen data. Therefore, it's important to use cross-validation techniques and regularization methods to prevent overfitting.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans07 : \n",
    "\"\"\"\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the dependent variable and one or more independent variables is modeled as an nth degree polynomial. This means that instead of modeling the relationship with a straight line (as in linear regression), we use a curve or a polynomial function to fit the data.\n",
    "The polynomial regression model can be expressed mathematically as:\n",
    "\n",
    "y = β0 + β1x + β2x^2 + β3x^3 + ... + βnx^n + ε\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable\n",
    "x is the independent variable\n",
    "β0, β1, β2, ..., βn are the coefficients of the polynomial terms, with β0 being the intercept term\n",
    "ε is the error term\n",
    "\n",
    "The difference between polynomial regression and linear regression is that linear regression models the relationship between the dependent variable and the independent variable as a straight line, while polynomial regression models the relationship as a curved line or a polynomial function.\n",
    "\n",
    "Polynomial regression can capture more complex relationships between the independent and dependent variables compared to linear regression. For example, if the data points are best fit with a curved line, polynomial regression can capture this relationship more accurately than a straight line in linear regression. This makes it a useful tool for modeling nonlinear relationships.\n",
    "One potential downside of polynomial regression is that it can be prone to overfitting. This means that the model may fit the training data too closely, which can lead to poor performance on new, unseen data. Therefore, it's important to use cross-validation techniques and regularization methods to prevent overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024204d0-c873-468f-a8f6-f242c793fd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n*Advantages of polynomial regression compared to linear regression:\\n\\nCapturing nonlinear relationships: Polynomial regression can capture more complex relationships between the independent and dependent variables compared to linear regression. If the data has a nonlinear relationship, polynomial regression can fit it more accurately than a straight line in linear regression.\\nFlexibility                      : Polynomial regression is a flexible modeling approach as it can be adjusted to fit various degrees of polynomial functions, allowing for more complex modeling of relationships.\\n\\n*Disadvantages of polynomial regression compared to linear regression:\\n\\nOverfitting  : Polynomial regression can be prone to overfitting, which is when the model fits the training data too closely, leading to poor performance on new data.\\nExtrapolation: Extrapolation in polynomial regression can be unreliable because the model is fit to the data within the range of the observed data. Predictions beyond that range can be inaccurate. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans08 :\n",
    "\"\"\"\n",
    "\n",
    "*Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Capturing nonlinear relationships: Polynomial regression can capture more complex relationships between the independent and dependent variables compared to linear regression. If the data has a nonlinear relationship, polynomial regression can fit it more accurately than a straight line in linear regression.\n",
    "Flexibility                      : Polynomial regression is a flexible modeling approach as it can be adjusted to fit various degrees of polynomial functions, allowing for more complex modeling of relationships.\n",
    "\n",
    "*Disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Overfitting  : Polynomial regression can be prone to overfitting, which is when the model fits the training data too closely, leading to poor performance on new data.\n",
    "Extrapolation: Extrapolation in polynomial regression can be unreliable because the model is fit to the data within the range of the observed data. Predictions beyond that range can be inaccurate. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c20cb-bc7a-46d9-989a-85ac987cedeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
