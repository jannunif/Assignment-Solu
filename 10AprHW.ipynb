{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb92d91-b087-480e-b23f-edf12469c8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo solve this problem, we can use Bayes' theorem, which states that:\\n\\nP(A|B) = P(B|A) * P(A) / P(B)\\n\\nwhere:\\n\\nP(A|B) is the probability of event A given event B (in our case, the probability that an employee is a smoker given that he/she uses the health insurance plan).\\nP(B|A) is the probability of event B given event A (in our case, the probability that an employee uses the health insurance plan given that he/she is a smoker). This is given as 40% or 0.4.\\nP(A) is the prior probability of event A (in our case, the probability that an employee is a smoker). This is not given, so we need to calculate it using the information given.\\nP(B) is the prior probability of event B (in our case, the probability that an employee uses the health insurance plan). This is given as 70% or 0.7.\\nTo find P(A), we can use the law of total probability, which states that the probability of an event can be found by considering all possible ways in which it can occur. In our case, we can consider two possible ways in which an employee can use the health insurance plan: either the employee is a smoker and uses the plan, or the employee is a non-smoker and uses the plan. Therefore:\\n\\nP(B) = P(Smoker and uses plan) + P(Non-smoker and uses plan)\\n= P(B|A) * P(A) + P(B|not A) * P(not A)\\n= 0.4 * P(A) + P(B|not A) * (1 - P(A))\\n\\nWe do not have information on the proportion of non-smokers who use the plan, but we know that all employees either smoke or do not smoke. Therefore, we can assume that:\\n\\nP(not A) = 1 - P(A)\\n\\nNow we can solve for P(A):\\n\\n0.7 = 0.4 * P(A) + P(B|not A) * (1 - P(A))\\n0.7 = 0.4 * P(A) + 0.7 * (1 - P(A))\\n0.7 = 0.7 - 0.3 * P(A)\\n0.3 * P(A) = 0\\nP(A) = 0\\n\\nThis result means that none of the employees smoke, which seems unlikely. Therefore, we need to revisit the assumptions and the data to make sure they are correct. If we assume that there is a positive probability of smoking, then we can use the corrected prior probability of smoking to calculate the conditional probability of smoking given the use of the health insurance plan:\\n\\nP(A) = 0.3 (for example)\\nP(A|B) = P(B|A) * P(A) / P(B)\\n= 0.4 * 0.3 / 0.7\\n= 0.1714 or 17.14%\\n\\nTherefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 17.14%.                                                                                                    \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans01: \n",
    "\"\"\"\n",
    "To solve this problem, we can use Bayes' theorem, which states that:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of event A given event B (in our case, the probability that an employee is a smoker given that he/she uses the health insurance plan).\n",
    "P(B|A) is the probability of event B given event A (in our case, the probability that an employee uses the health insurance plan given that he/she is a smoker). This is given as 40% or 0.4.\n",
    "P(A) is the prior probability of event A (in our case, the probability that an employee is a smoker). This is not given, so we need to calculate it using the information given.\n",
    "P(B) is the prior probability of event B (in our case, the probability that an employee uses the health insurance plan). This is given as 70% or 0.7.\n",
    "To find P(A), we can use the law of total probability, which states that the probability of an event can be found by considering all possible ways in which it can occur. In our case, we can consider two possible ways in which an employee can use the health insurance plan: either the employee is a smoker and uses the plan, or the employee is a non-smoker and uses the plan. Therefore:\n",
    "\n",
    "P(B) = P(Smoker and uses plan) + P(Non-smoker and uses plan)\n",
    "= P(B|A) * P(A) + P(B|not A) * P(not A)\n",
    "= 0.4 * P(A) + P(B|not A) * (1 - P(A))\n",
    "\n",
    "We do not have information on the proportion of non-smokers who use the plan, but we know that all employees either smoke or do not smoke. Therefore, we can assume that:\n",
    "\n",
    "P(not A) = 1 - P(A)\n",
    "\n",
    "Now we can solve for P(A):\n",
    "\n",
    "0.7 = 0.4 * P(A) + P(B|not A) * (1 - P(A))\n",
    "0.7 = 0.4 * P(A) + 0.7 * (1 - P(A))\n",
    "0.7 = 0.7 - 0.3 * P(A)\n",
    "0.3 * P(A) = 0\n",
    "P(A) = 0\n",
    "\n",
    "This result means that none of the employees smoke, which seems unlikely. Therefore, we need to revisit the assumptions and the data to make sure they are correct. If we assume that there is a positive probability of smoking, then we can use the corrected prior probability of smoking to calculate the conditional probability of smoking given the use of the health insurance plan:\n",
    "\n",
    "P(A) = 0.3 (for example)\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "= 0.4 * 0.3 / 0.7\n",
    "= 0.1714 or 17.14%\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 17.14%.                                                                                                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5458d630-b808-4760-9530-00c95e8d22f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of data they are suitable for.\\nBernoulli Naive Bayes is typically used for binary data, where each feature can take on only one of two possible values (e.g., 0 or 1). It models the conditional probability of each feature given the class as a Bernoulli distribution.\\nMultinomial Naive Bayes, on the other hand, is used for data with discrete counts, such as word counts in text classification. It models the conditional probability of each feature given the class as a Multinomial distribution.\\nIn Bernoulli Naive Bayes, the presence or absence of a feature is used as a predictor, while in Multinomial Naive Bayes, the frequency of the feature is used.                      '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans02: \n",
    "\"\"\"\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of data they are suitable for.\n",
    "Bernoulli Naive Bayes is typically used for binary data, where each feature can take on only one of two possible values (e.g., 0 or 1). It models the conditional probability of each feature given the class as a Bernoulli distribution.\n",
    "Multinomial Naive Bayes, on the other hand, is used for data with discrete counts, such as word counts in text classification. It models the conditional probability of each feature given the class as a Multinomial distribution.\n",
    "In Bernoulli Naive Bayes, the presence or absence of a feature is used as a predictor, while in Multinomial Naive Bayes, the frequency of the feature is used.                      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2a9170-ee0e-4fe9-8d27-90815e1dbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans03: Bernoulli Naive Bayes assumes that all features are binary, i.e., taking values of 0 or 1. If a feature is missing, it is typically treated as if it were 0. This is because a missing feature can be thought of as the absence of evidence, and the absence of evidence is usually considered evidence of absence in a binary classification problem. However, this assumption may not always be appropriate, and different methods such as imputation or modeling the missing data explicitly may be necessary depending on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412ca16a-8d54-439e-a506-4ebfa3a7efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans04: Yes, Gaussian Naive Bayes can be used for multi-class classification. One common approach is to use the \"one-vs-all\" or \"one-vs-rest\" strategy, where separate binary classifiers are trained for each class, with each classifier separating that class from all the other classes. During prediction, the class with the highest probability is chosen as the predicted class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
