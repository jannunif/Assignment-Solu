{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281360d1-6680-4603-9042-08bcaf2f7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans01: Bayes' theorem states that the probability of an event A, given some evidence B, is equal to the probability of the evidence given the event multiplied by the prior probability of the event, divided by the marginal probability of the evidence:\n",
    "\n",
    "# P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955fac60-7a9c-422d-9a3c-a1bf7a4abb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans02: P(A|B) = P(B|A) * P(A) / P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89593c21-10e5-4c05-bbae-c6545b514eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBayes' theorem is used in a wide range of practical applications, especially in fields that involve uncertainty and decision-making. Here are some examples:\\n\\n1) Medical diagnosis          : Bayes' theorem can be used to calculate the probability of a patient having a disease given their symptoms, test results, and other medical information. This can help doctors make more informed decisions about diagnosis and treatment.\\n2) Spam filtering             : Bayes' theorem can be used to classify emails as spam or not spam based on the frequency of certain words and phrases in the email. This can help users filter out unwanted emails and reduce the risk of phishing attacks.\\n3) Credit scoring             : Bayes' theorem can be used to predict the likelihood of a borrower defaulting on a loan based on their credit history, income, and other factors. This can help lenders make more informed decisions about loan approvals and interest rates.\\n4) Fault diagnosis            : Bayes' theorem can be used to identify faults in complex systems such as industrial equipment and aircraft. By analyzing sensor data and other information, the system can identify the most likely causes of a fault and take corrective action.\\n5) Natural language processing: Bayes' theorem can be used to classify text data, such as news articles or social media posts, into different categories based on the frequency of certain words and phrases. This can help with tasks such as sentiment analysis and topic modeling.      \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans03:\n",
    "\"\"\"\n",
    "Bayes' theorem is used in a wide range of practical applications, especially in fields that involve uncertainty and decision-making. Here are some examples:\n",
    "\n",
    "1) Medical diagnosis          : Bayes' theorem can be used to calculate the probability of a patient having a disease given their symptoms, test results, and other medical information. This can help doctors make more informed decisions about diagnosis and treatment.\n",
    "2) Spam filtering             : Bayes' theorem can be used to classify emails as spam or not spam based on the frequency of certain words and phrases in the email. This can help users filter out unwanted emails and reduce the risk of phishing attacks.\n",
    "3) Credit scoring             : Bayes' theorem can be used to predict the likelihood of a borrower defaulting on a loan based on their credit history, income, and other factors. This can help lenders make more informed decisions about loan approvals and interest rates.\n",
    "4) Fault diagnosis            : Bayes' theorem can be used to identify faults in complex systems such as industrial equipment and aircraft. By analyzing sensor data and other information, the system can identify the most likely causes of a fault and take corrective action.\n",
    "5) Natural language processing: Bayes' theorem can be used to classify text data, such as news articles or social media posts, into different categories based on the frequency of certain words and phrases. This can help with tasks such as sentiment analysis and topic modeling.      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2174b42-9f3d-4267-935b-b79a27bb440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n*Bayes' theorem is a mathematical formula that relates conditional probabilities. It provides a way to update the probability of an event based on new evidence or information. Specifically, Bayes' theorem states that:\\n\\nP(A|B) = P(B|A) * P(A) / P(B)\\n\\nwhere:\\n\\nP(A|B) is the probability of event A given event B\\nP(B|A) is the probability of event B given event A\\nP(A) is the prior probability of event A\\nP(B) is the prior probability of event B\\nThis formula allows us to calculate the probability of A given B, which is called the posterior probability, based on the probability of B given A and the prior probabilities of A and B.\\n\\n*Conditional probability, on the other hand, is a basic concept in probability theory that describes the probability of an event given that another event has occurred. It is defined as:\\n\\nP(A|B) = P(A and B) / P(B)\\n\\nwhere:\\n\\nP(A and B) is the probability of A and B occurring together\\nP(B) is the probability of event B\\nConditional probability is a fundamental concept in Bayes' theorem, as it provides the starting point for calculating the posterior probability. Bayes' theorem allows us to update our beliefs about the probability of an event based on new information or evidence, and is a powerful tool for modeling uncertainty and making decisions based on incomplete or noisy data. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans04:\n",
    "\"\"\"\n",
    "*Bayes' theorem is a mathematical formula that relates conditional probabilities. It provides a way to update the probability of an event based on new evidence or information. Specifically, Bayes' theorem states that:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of event A given event B\n",
    "P(B|A) is the probability of event B given event A\n",
    "P(A) is the prior probability of event A\n",
    "P(B) is the prior probability of event B\n",
    "This formula allows us to calculate the probability of A given B, which is called the posterior probability, based on the probability of B given A and the prior probabilities of A and B.\n",
    "\n",
    "*Conditional probability, on the other hand, is a basic concept in probability theory that describes the probability of an event given that another event has occurred. It is defined as:\n",
    "\n",
    "P(A|B) = P(A and B) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A and B) is the probability of A and B occurring together\n",
    "P(B) is the probability of event B\n",
    "Conditional probability is a fundamental concept in Bayes' theorem, as it provides the starting point for calculating the posterior probability. Bayes' theorem allows us to update our beliefs about the probability of an event based on new information or evidence, and is a powerful tool for modeling uncertainty and making decisions based on incomplete or noisy data. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30500bc5-bafd-4b17-8913-6f621e691895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChoosing the appropriate type of Naive Bayes classifier to use for a given problem depends on the nature of the data and the assumptions that can be made about the distribution of the features.\\nGaussian Naive Bayes: This classifier is suitable for continuous numerical data that can be modeled using a Gaussian (normal) distribution. If the features are continuous and follow a bell-shaped curve, Gaussian Naive Bayes may be a good choice.\\nMultinomial Naive Bayes: This classifier is suitable for discrete count data, such as word frequencies in text classification problems. If the features are counts or frequencies of discrete events, Multinomial Naive Bayes may be a good choice.\\nBernoulli Naive Bayes: This classifier is also suitable for discrete data, but assumes that the features are binary (i.e., they take on values of 0 or 1). If the features are binary indicators of the presence or absence of certain events, Bernoulli Naive Bayes may be a good choice.    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans05 : \n",
    "\"\"\"\n",
    "Choosing the appropriate type of Naive Bayes classifier to use for a given problem depends on the nature of the data and the assumptions that can be made about the distribution of the features.\n",
    "Gaussian Naive Bayes: This classifier is suitable for continuous numerical data that can be modeled using a Gaussian (normal) distribution. If the features are continuous and follow a bell-shaped curve, Gaussian Naive Bayes may be a good choice.\n",
    "Multinomial Naive Bayes: This classifier is suitable for discrete count data, such as word frequencies in text classification problems. If the features are counts or frequencies of discrete events, Multinomial Naive Bayes may be a good choice.\n",
    "Bernoulli Naive Bayes: This classifier is also suitable for discrete data, but assumes that the features are binary (i.e., they take on values of 0 or 1). If the features are binary indicators of the presence or absence of certain events, Bernoulli Naive Bayes may be a good choice.    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdef4ac-f8fb-4d1e-aba8-ce7b2df10eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTo predict the class for the new instance with features X1=3 and X2=4, we need to calculate the posterior probability of each class given the observed feature values.\\nUsing Bayes' theorem, the posterior probability of class A given X1=3 and X2=4 is:\\nP(A | X1=3, X2=4) = P(X1=3, X2=4 | A) * P(A) / P(X1=3, X2=4)\\nAnd the posterior probability of class B given X1=3 and X2=4 is:\\nP(B | X1=3, X2=4) = P(X1=3, X2=4 | B) * P(B) / P(X1=3, X2=4)\\n\\nSince we are assuming equal prior probabilities for each class, P(A) = P(B) = 0.5.\\n\\nTo calculate P(X1=3, X2=4 | A), we need to find the frequency of instances in class A with features X1=3 and X2=4:\\nP(X1=3, X2=4 | A) = 1/16\\n\\nSimilarly, we can calculate P(X1=3, X2=4 | B):\\nP(X1=3, X2=4 | B) = 3/16\\n\\nTo calculate P(X1=3, X2=4), we can use the law of total probability:\\nP(X1=3, X2=4) = P(X1=3, X2=4 | A) * P(A) + P(X1=3, X2=4 | B) * P(B)\\nP(X1=3, X2=4) = (1/16) * 0.5 + (3/16) * 0.5\\nP(X1=3, X2=4) = 0.25\\n\\nNow we can calculate the posterior probabilities for each class:\\n\\nP(A | X1=3, X2=4) = (1/16) * 0.5 / 0.25 = 0.25\\nP(B | X1=3, X2=4) = (3/16) * 0.5 / 0.25 = 0.75\\n\\nTherefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class B.             \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans06:\n",
    "\"\"\"\n",
    "To predict the class for the new instance with features X1=3 and X2=4, we need to calculate the posterior probability of each class given the observed feature values.\n",
    "Using Bayes' theorem, the posterior probability of class A given X1=3 and X2=4 is:\n",
    "P(A | X1=3, X2=4) = P(X1=3, X2=4 | A) * P(A) / P(X1=3, X2=4)\n",
    "And the posterior probability of class B given X1=3 and X2=4 is:\n",
    "P(B | X1=3, X2=4) = P(X1=3, X2=4 | B) * P(B) / P(X1=3, X2=4)\n",
    "\n",
    "Since we are assuming equal prior probabilities for each class, P(A) = P(B) = 0.5.\n",
    "\n",
    "To calculate P(X1=3, X2=4 | A), we need to find the frequency of instances in class A with features X1=3 and X2=4:\n",
    "P(X1=3, X2=4 | A) = 1/16\n",
    "\n",
    "Similarly, we can calculate P(X1=3, X2=4 | B):\n",
    "P(X1=3, X2=4 | B) = 3/16\n",
    "\n",
    "To calculate P(X1=3, X2=4), we can use the law of total probability:\n",
    "P(X1=3, X2=4) = P(X1=3, X2=4 | A) * P(A) + P(X1=3, X2=4 | B) * P(B)\n",
    "P(X1=3, X2=4) = (1/16) * 0.5 + (3/16) * 0.5\n",
    "P(X1=3, X2=4) = 0.25\n",
    "\n",
    "Now we can calculate the posterior probabilities for each class:\n",
    "\n",
    "P(A | X1=3, X2=4) = (1/16) * 0.5 / 0.25 = 0.25\n",
    "P(B | X1=3, X2=4) = (3/16) * 0.5 / 0.25 = 0.75\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class B.             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae06c5-05ce-430a-86c8-d0ba4869ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
