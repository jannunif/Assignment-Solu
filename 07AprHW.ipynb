{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8f79b8-e6ca-4d7f-8281-cd3195d82294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPolynomial functions and kernel functions are both used in machine learning algorithms, but they serve different purposes.\\nPolynomial functions are a class of functions that can be expressed as a sum of powers of a variable, with coefficients for each power. They are often used as feature transformations in machine learning, where the original data is mapped to a higher-dimensional feature space using polynomial functions. This can be useful when the original data is not linearly separable and a higher-dimensional space allows for better separation.\\nKernel functions, on the other hand, are used in kernel methods, which are a class of machine learning algorithms that operate in the feature space. A kernel function is a function that takes two inputs and returns a scalar value that represents the similarity between the inputs in the feature space. The kernel function allows the algorithm to work with the data in the feature space without explicitly computing the transformed feature vectors.\\nPolynomial functions can be used as kernel functions in some kernel methods, such as the polynomial kernel. In this case, the polynomial function is used to compute the similarity between the feature vectors in the higher-dimensional space. However, there are many other types of kernel functions, such as the Gaussian kernel and the Laplacian kernel, which are not based on polynomial functions. '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans01:\n",
    "\"\"\"\n",
    "Polynomial functions and kernel functions are both used in machine learning algorithms, but they serve different purposes.\n",
    "Polynomial functions are a class of functions that can be expressed as a sum of powers of a variable, with coefficients for each power. They are often used as feature transformations in machine learning, where the original data is mapped to a higher-dimensional feature space using polynomial functions. This can be useful when the original data is not linearly separable and a higher-dimensional space allows for better separation.\n",
    "Kernel functions, on the other hand, are used in kernel methods, which are a class of machine learning algorithms that operate in the feature space. A kernel function is a function that takes two inputs and returns a scalar value that represents the similarity between the inputs in the feature space. The kernel function allows the algorithm to work with the data in the feature space without explicitly computing the transformed feature vectors.\n",
    "Polynomial functions can be used as kernel functions in some kernel methods, such as the polynomial kernel. In this case, the polynomial function is used to compute the similarity between the feature vectors in the higher-dimensional space. However, there are many other types of kernel functions, such as the Gaussian kernel and the Laplacian kernel, which are not based on polynomial functions. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25409075-b4d0-4dcc-8481-5c1c74d9ef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Ans02: \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM model with a polynomial kernel\n",
    "model = SVC(kernel='poly', degree=3, coef0=1, C=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: %.2f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec72e5c3-b493-4acb-ad14-6a099b9af741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn support vector regression (SVR), the value of epsilon determines the width of the margin, which is the range within which errors are considered acceptable and not penalized. Increasing the value of epsilon makes the margin wider, which allows more training samples to be within the margin and hence not contribute to the determination of the decision boundary.\\nAs the margin gets wider, the number of support vectors typically increases because more training samples may fall within the margin or even on the wrong side of the decision boundary. This means that the model becomes less sensitive to individual training samples and more generalizable to the overall trend of the data.\\nTherefore, increasing the value of epsilon tends to increase the number of support vectors in SVR. However, this may also lead to decreased model complexity and possibly lower accuracy, as the model may underfit the training data by not capturing the complexity of the underlying relationship. It is important to choose an appropriate value of epsilon based on the specific problem and data at hand, as well as to perform proper model evaluation and selection to avoid overfitting or underfitting.  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans03:\n",
    "\"\"\"\n",
    "In support vector regression (SVR), the value of epsilon determines the width of the margin, which is the range within which errors are considered acceptable and not penalized. Increasing the value of epsilon makes the margin wider, which allows more training samples to be within the margin and hence not contribute to the determination of the decision boundary.\n",
    "As the margin gets wider, the number of support vectors typically increases because more training samples may fall within the margin or even on the wrong side of the decision boundary. This means that the model becomes less sensitive to individual training samples and more generalizable to the overall trend of the data.\n",
    "Therefore, increasing the value of epsilon tends to increase the number of support vectors in SVR. However, this may also lead to decreased model complexity and possibly lower accuracy, as the model may underfit the training data by not capturing the complexity of the underlying relationship. It is important to choose an appropriate value of epsilon based on the specific problem and data at hand, as well as to perform proper model evaluation and selection to avoid overfitting or underfitting.  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78225087-2cc8-4295-9d98-f24451c396f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can have a significant impact on the performance of the model. Here is an explanation of each parameter and how it affects the SVR model:\\n\\n1)Kernel Function  : The kernel function is used to transform the input data into a higher-dimensional feature space where the relationship between the input variables and the output variable is more linearly separable. The most commonly used kernel functions are linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel function depends on the nature of the data and the problem at hand. For example, the linear kernel is suitable for linearly separable data, while the RBF kernel is more appropriate for non-linear data.\\n2)C Parameter      : The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A large value of C allows more training errors, but results in a smaller margin. Conversely, a small value of C results in a larger margin but fewer training errors. A high value of C may lead to overfitting, while a low value of C may result in underfitting. Therefore, the value of C should be chosen based on the complexity of the problem and the amount of noise in the data.\\n3)Epsilon Parameter: The epsilon parameter controls the width of the margin, which is the region where errors are not penalized. A larger value of epsilon results in a wider margin and fewer support vectors, which can lead to a simpler model. Conversely, a smaller value of epsilon leads to a narrower margin and more support vectors, which can result in a more complex model. The choice of epsilon depends on the level of noise in the data and the desired trade-off between model complexity and accuracy.\\n4)Gamma Parameter  : The gamma parameter controls the shape of the decision boundary and the influence of each training sample. A high value of gamma leads to a tighter decision boundary, which may result in overfitting. Conversely, a low value of gamma results in a smoother decision boundary, which may lead to underfitting. The value of gamma should be chosen based on the nature of the data and the complexity of the problem.  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans04: \n",
    "\"\"\"\n",
    "In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can have a significant impact on the performance of the model. Here is an explanation of each parameter and how it affects the SVR model:\n",
    "\n",
    "1)Kernel Function  : The kernel function is used to transform the input data into a higher-dimensional feature space where the relationship between the input variables and the output variable is more linearly separable. The most commonly used kernel functions are linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel function depends on the nature of the data and the problem at hand. For example, the linear kernel is suitable for linearly separable data, while the RBF kernel is more appropriate for non-linear data.\n",
    "2)C Parameter      : The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A large value of C allows more training errors, but results in a smaller margin. Conversely, a small value of C results in a larger margin but fewer training errors. A high value of C may lead to overfitting, while a low value of C may result in underfitting. Therefore, the value of C should be chosen based on the complexity of the problem and the amount of noise in the data.\n",
    "3)Epsilon Parameter: The epsilon parameter controls the width of the margin, which is the region where errors are not penalized. A larger value of epsilon results in a wider margin and fewer support vectors, which can lead to a simpler model. Conversely, a smaller value of epsilon leads to a narrower margin and more support vectors, which can result in a more complex model. The choice of epsilon depends on the level of noise in the data and the desired trade-off between model complexity and accuracy.\n",
    "4)Gamma Parameter  : The gamma parameter controls the shape of the decision boundary and the influence of each training sample. A high value of gamma leads to a tighter decision boundary, which may result in overfitting. Conversely, a low value of gamma results in a smoother decision boundary, which may lead to underfitting. The value of gamma should be chosen based on the nature of the data and the complexity of the problem.  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b274688-c566-4fa1-8cf5-3cc3ca2a6c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "F1-score: 0.99\n",
      "Best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best score: 0.9758241758241759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['breast_cancer_classifier.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ans05:\n",
    "\n",
    "# Import the necessary libraries and load the dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "classifier = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy and F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('Accuracy: %.2f' % accuracy)\n",
    "print('F1-score: %.2f' % f1)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(classifier, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best score:', grid_search.best_score_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "classifier_tuned = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)\n",
    "classifier_tuned.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(classifier_tuned, 'breast_cancer_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bc817-e71f-4b87-b1bc-156d4e2017c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
